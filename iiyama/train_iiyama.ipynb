{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.externals import joblib\n",
    "from user_function import MyAlgorithm\n",
    "from alcon_utils import AlconUtils\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def read_data(datasetdir):\n",
    "    alcon = AlconUtils(datasetdir)\n",
    "\n",
    "    # アノテーションの読み込み\n",
    "    fn = \"target_lv1.csv\"\n",
    "    alcon.load_annotations_target(fn)\n",
    "\n",
    "    fn = \"groundtruth_lv1.csv\"\n",
    "    alcon.load_annotations_ground(fn)\n",
    "\n",
    "    # KNNモデルの作成\n",
    "    dataset = {}\n",
    "    for bb_id, target in alcon.targets.items():\n",
    "        img_filename = alcon.get_filename_char( bb_id )\n",
    "        code = alcon.ground_truth[bb_id][0]\n",
    "        if code not in dataset:\n",
    "            dataset[code] = []\n",
    "        if len(dataset[code]) == 10:\n",
    "            continue\n",
    "        img = cv2.imread( img_filename )\n",
    "        feature = MyAlgorithm.feature_extraction(img)\n",
    "        dataset[code].append(feature)\n",
    "\n",
    "    labels = []\n",
    "    data = []\n",
    "    classes = sorted(dataset.keys())\n",
    "    for label, values in dataset.items():\n",
    "        labels += [classes.index(label)] * len(values)\n",
    "        data += values\n",
    "\n",
    "    data = np.asarray(data, dtype=np.float)\n",
    "    labels = np.asarray(labels, dtype=np.int)\n",
    "    \n",
    "    return data, labels\n",
    "    \n",
    "def main(datasetdir,lv):\n",
    "    data, labels = read_data(datasetdir,lv)\n",
    "    classifier = KNeighborsClassifier()\n",
    "    classifier.fit(data, labels)\n",
    "\n",
    "    outputfile = \"./model.pkl\"\n",
    "    joblib.dump((classes, classifier), outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "def main_keras_NN(datasetdir):\n",
    "    data, labels = read_data(datasetdir)\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "    K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "    \n",
    "    batch_size = 128\n",
    "    num_classes = 46\n",
    "    epochs = 1\n",
    "    img_rows, img_cols = 32, 32\n",
    "    \n",
    "    data = data.reshape(data.shape[0],img_rows,img_cols,3)\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "    data = data.astype('float32')\n",
    "    labels = keras.utils.to_categorical(labels, num_classes)\n",
    "    \n",
    "    classifier = Sequential()\n",
    "    \n",
    "    classifier.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape,padding='same'))\n",
    "    classifier.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    classifier.add(Dropout(0.25))\n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dense(128,activation='relu'))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    classifier.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    classifier.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                       optimizer=keras.optimizers.Adadelta(),\n",
    "                       metrics=['accuracy'])\n",
    "    \n",
    "    classifier.fit(data,labels,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=None)\n",
    "\n",
    "    outputfile = \"./model.pkl\"\n",
    "    classifier.save(outputfile)\n",
    "\n",
    "    outputfile2 = \"./classes.pkl\"\n",
    "    joblib.dump(classes, outputfile2)\n",
    "    \n",
    "    print (classes)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"usage: python train.py datasetdir lv\", file=sys.stderr)\n",
    "        quit()\n",
    "\n",
    "    main(sys.argv[1], sys.argv[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "main_keras_NN(\"/share/alcon/dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
