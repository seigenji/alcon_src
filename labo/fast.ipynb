{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import sys\n",
    "from sklearn.externals import joblib\n",
    "from user_function import MyAlgorithm\n",
    "from alcon_utils import AlconUtils\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow\n",
    "import cProfile, pstats\n",
    "\n",
    "import os.path\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "config = tensorflow.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.93\n",
    "keras.backend.tensorflow_backend.set_session(tensorflow.Session(config=config))\n",
    "\n",
    "# 初期化\n",
    "datasetdir = \"/share/alcon/dataset/\"\n",
    "alcon = AlconUtils(datasetdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         482548 function calls in 113.226 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 51 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    12998   89.744    0.007   89.744    0.007 {bilateralFilter}\n",
      "    12998   22.272    0.002   22.272    0.002 {imread}\n",
      "    12998    0.197    0.000    0.197    0.000 {cvtColor}\n",
      "    12998    0.196    0.000    0.196    0.000 {threshold}\n",
      "        1    0.171    0.171  113.007  113.007 <ipython-input-45-3403e1b54a1d>:9(<module>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7f5c1f7a9710>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "# アノテーションの読み込み\n",
    "alcon.load_annotations_target(\"target_lv1_iiyama_0.9.csv\")\n",
    "alcon.load_annotations_ground(\"groundtruth_lv1_iiyama_0.9.csv\")\n",
    "\n",
    "dataset = {}\n",
    "for bb_id, target in alcon.targets.items():\n",
    "    code = alcon.ground_truth[bb_id][0]\n",
    "    if code not in dataset:\n",
    "        dataset[code] = []\n",
    "    if len(dataset[code]) == 300:\n",
    "        continue\n",
    "    img_filename = alcon.get_filename_char( bb_id )\n",
    "    img = cv2.imread( img_filename )\n",
    "    feature = MyAlgorithm.feature_extraction(img)\n",
    "    dataset[code].append(feature)\n",
    "\n",
    "labels = []\n",
    "data = []\n",
    "classes = sorted(dataset.keys())\n",
    "\n",
    "for label, values in dataset.items():\n",
    "    labels += [classes.index(label)] * len(values)\n",
    "    data += values\n",
    "\n",
    "num_classes = 46\n",
    "input_shape = (32, 32, 1)# img_rows img_cols channel\n",
    "\n",
    "x_data = np.asarray(data).reshape(len(data), *input_shape)\n",
    "y_train = keras.utils.to_categorical( labels, num_classes )\n",
    "datasetdir = \"/share/alcon/dataset/\"\n",
    "annotation_name = \"test_5\"\n",
    "\n",
    "# 初期化\n",
    "alcon = AlconUtils(datasetdir)\n",
    "myalgorithm = MyAlgorithm(datasetdir)\n",
    "\n",
    "# ターゲットの読み込み\n",
    "file_name_last = \"_lv1_\" + annotation_name + \".csv\"\n",
    "alcon.load_annotations_target(\"target\" + file_name_last)\n",
    "\n",
    "pr.disable()\n",
    "pstats.Stats(pr).sort_stats('tottime').print_stats(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         849849 function calls (831713 primitive calls) in 4.271 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 1049 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        5    1.607    0.321    1.607    0.321 {built-in method _pywrap_tensorflow_internal.TF_Run}\n",
      "        1    0.406    0.406    0.407    0.407 /root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/scipy/linalg/decomp_svd.py:16(svd)\n",
      "      578    0.175    0.000    0.196    0.000 {built-in method numpy.core.multiarray.array}\n",
      "        1    0.169    0.169    0.169    0.169 {built-in method _pickle.load}\n",
      "        3    0.158    0.053    0.158    0.053 {built-in method numpy.core.multiarray.dot}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7ff01c647588>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#　学習機に掛ける前の準備\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "config = tensorflow.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.93\n",
    "keras.backend.tensorflow_backend.set_session(tensorflow.Session(config=config))\n",
    "\n",
    "# 初期化\n",
    "datasetdir = \"/share/alcon/dataset/\"\n",
    "alcon = AlconUtils(datasetdir)\n",
    "\n",
    "# アノテーションの読み込み\n",
    "alcon.load_annotations_target(\"target_lv1_iiyama_0.9.csv\")\n",
    "alcon.load_annotations_ground(\"groundtruth_lv1_iiyama_0.9.csv\")\n",
    "\n",
    "dataset = {}\n",
    "with open('full_dataset0.9', mode='rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "labels = []\n",
    "data = []\n",
    "classes = sorted(dataset.keys())\n",
    "\n",
    "for label, values in dataset.items():\n",
    "    labels += [classes.index(label)] * len(values)\n",
    "    data += values\n",
    "\n",
    "num_classes = 46\n",
    "input_shape = (32, 32, 1)# img_rows img_cols channel\n",
    "datagen = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    zca_whitening=True\n",
    "    #rotation_range=20,\n",
    "    #width_shift_range=0.1,\n",
    "    #height_shift_range=0.1,\n",
    "    #shear_range=0.3,\n",
    "    #zoom_range=0.1,\n",
    "    #fill_mode='constant',\n",
    "    #cval=0.\n",
    ")\n",
    "x_train = np.asarray(data).reshape(len(data), *input_shape)\n",
    "y_train = keras.utils.to_categorical( labels, num_classes )\n",
    "datagen.fit(x_train)\n",
    "datasetdir = \"/share/alcon/dataset/\"\n",
    "annotation_name = \"test_5\"\n",
    "\n",
    "# 初期化\n",
    "alcon = AlconUtils(datasetdir)\n",
    "myalgorithm = MyAlgorithm(datasetdir)\n",
    "\n",
    "# ターゲットの読み込み\n",
    "file_name_last = \"_lv1_\" + annotation_name + \".csv\"\n",
    "alcon.load_annotations_target(\"target\" + file_name_last)\n",
    "\n",
    "pr.disable()\n",
    "pstats.Stats(pr).sort_stats('tottime').print_stats(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44353 samples, validate on 44353 samples\n",
      "Epoch 1/50\n",
      "44353/44353 [==============================] - 11s - loss: 1.4227 - acc: 0.6345 - val_loss: 1.1355 - val_acc: 0.7179\n",
      "Epoch 2/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.6983 - acc: 0.8147 - val_loss: 0.5912 - val_acc: 0.8733\n",
      "Epoch 3/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.5175 - acc: 0.8600 - val_loss: 0.3755 - val_acc: 0.9199\n",
      "Epoch 4/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.4257 - acc: 0.8827 - val_loss: 0.2772 - val_acc: 0.9586\n",
      "Epoch 5/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.3549 - acc: 0.9030 - val_loss: 0.1819 - val_acc: 0.9699\n",
      "Epoch 6/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.3135 - acc: 0.9125 - val_loss: 0.1610 - val_acc: 0.9747\n",
      "Epoch 7/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.2766 - acc: 0.9229 - val_loss: 0.1457 - val_acc: 0.9779\n",
      "Epoch 8/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.2560 - acc: 0.9279 - val_loss: 0.1098 - val_acc: 0.9819\n",
      "Epoch 9/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.2233 - acc: 0.9355 - val_loss: 0.0917 - val_acc: 0.9863\n",
      "Epoch 10/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.2130 - acc: 0.9386 - val_loss: 0.0841 - val_acc: 0.9878\n",
      "Epoch 11/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1988 - acc: 0.9427 - val_loss: 0.0880 - val_acc: 0.9905\n",
      "Epoch 12/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1920 - acc: 0.9456 - val_loss: 0.0777 - val_acc: 0.9904\n",
      "Epoch 13/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1789 - acc: 0.9486 - val_loss: 0.0627 - val_acc: 0.9914\n",
      "Epoch 14/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1645 - acc: 0.9522 - val_loss: 0.0544 - val_acc: 0.9934\n",
      "Epoch 15/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1594 - acc: 0.9543 - val_loss: 0.0597 - val_acc: 0.9922\n",
      "Epoch 16/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1590 - acc: 0.9541 - val_loss: 0.0510 - val_acc: 0.9946\n",
      "Epoch 17/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1431 - acc: 0.9592 - val_loss: 0.0364 - val_acc: 0.9958\n",
      "Epoch 18/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1405 - acc: 0.9587 - val_loss: 0.0325 - val_acc: 0.9969\n",
      "Epoch 19/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1327 - acc: 0.9636 - val_loss: 0.0805 - val_acc: 0.9861\n",
      "Epoch 20/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1398 - acc: 0.9597 - val_loss: 0.0304 - val_acc: 0.9966\n",
      "Epoch 21/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1251 - acc: 0.9630 - val_loss: 0.0300 - val_acc: 0.9970\n",
      "Epoch 22/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1223 - acc: 0.9647 - val_loss: 0.0290 - val_acc: 0.9963\n",
      "Epoch 23/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1145 - acc: 0.9670 - val_loss: 0.0209 - val_acc: 0.9977\n",
      "Epoch 24/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1110 - acc: 0.9686 - val_loss: 0.0265 - val_acc: 0.9975\n",
      "Epoch 25/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1105 - acc: 0.9680 - val_loss: 0.0225 - val_acc: 0.9983\n",
      "Epoch 26/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1086 - acc: 0.9682 - val_loss: 0.0205 - val_acc: 0.9975\n",
      "Epoch 27/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1070 - acc: 0.9684 - val_loss: 0.0199 - val_acc: 0.9979\n",
      "Epoch 28/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1076 - acc: 0.9692 - val_loss: 0.0204 - val_acc: 0.9983\n",
      "Epoch 29/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1038 - acc: 0.9704 - val_loss: 0.0126 - val_acc: 0.9988\n",
      "Epoch 30/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.0999 - acc: 0.9711 - val_loss: 0.0159 - val_acc: 0.9986\n",
      "Epoch 31/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.1008 - acc: 0.9714 - val_loss: 0.0214 - val_acc: 0.9977\n",
      "Epoch 32/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.0966 - acc: 0.9722 - val_loss: 0.0125 - val_acc: 0.9993\n",
      "Epoch 33/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.0911 - acc: 0.9735 - val_loss: 0.0230 - val_acc: 0.9982\n",
      "Epoch 34/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.0934 - acc: 0.9738 - val_loss: 0.0119 - val_acc: 0.9989\n",
      "Epoch 35/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.0972 - acc: 0.9732 - val_loss: 0.0155 - val_acc: 0.9985\n",
      "Epoch 36/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.0866 - acc: 0.9747 - val_loss: 0.0507 - val_acc: 0.9921\n",
      "Epoch 37/50\n",
      "44353/44353 [==============================] - 11s - loss: 0.0960 - acc: 0.9738 - val_loss: 0.0149 - val_acc: 0.9989\n",
      "results: 223 / 230\n",
      "score: 96.956522\n",
      "         34119998 function calls (33984720 primitive calls) in 437.386 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 1136 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    39387  352.505    0.009  352.505    0.009 {built-in method _pywrap_tensorflow_internal.TF_Run}\n",
      "    39376    6.215    0.000    6.215    0.000 /root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/engine/training.py:394(<listcomp>)\n",
      "   314197    5.218    0.000    5.218    0.000 {built-in method numpy.core.multiarray.array}\n",
      "    39387    4.389    0.000  390.153    0.010 /root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/tensorflow/python/client/session.py:903(_run)\n",
      "  8609810    3.799    0.000    3.800    0.000 {built-in method builtins.hasattr}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7fef8ddc1518>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "classifier = keras.models.Sequential()\n",
    "classifier.add(keras.layers.normalization.BatchNormalization(input_shape = input_shape))\n",
    "classifier.add(Conv2D(32, kernel_size=(3,3), activation='relu')) # 30*30\n",
    "classifier.add(Conv2D(64,                 (3,3), activation='relu')) # 28*28 \n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(MaxPooling2D(pool_size=(4,4)))                      # 7*7\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(480, activation='relu'))\n",
    "classifier.add(Dropout(7/24))\n",
    "classifier.add(Dense(128, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "classifier.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Nadam(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = classifier.fit(x_train, y_train, batch_size= 84, epochs=50,\n",
    "                      callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)],\n",
    "                      validation_data=(x_train, y_train))\n",
    "\"\"\"\n",
    "history = classifier.fit_generator(datagen.flow(x_train, y_train, batch_size=500),\n",
    "                                         steps_per_epoch=len(x_train))\n",
    "\n",
    "                                         #epochs = 1\n",
    "\"\"\"\n",
    "plt.plot(history.history['acc'])\n",
    "\n",
    "joblib.dump(classes, \"./model.pkl\")\n",
    "classifier.save(\"./model2.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for bb_id, target in alcon.targets.items():\n",
    "    img_file_id, *bb = target\n",
    "    # 認識処理（ここで各自のアルゴリズムを呼び出します）\n",
    "    # boundingbox id と紐付けて保存\n",
    "    results[bb_id] = myalgorithm.predict(imgs[bb_id], bb)\n",
    "\n",
    "\n",
    "# 評価\n",
    "alcon.load_annotations_ground(\"groundtruth\" + file_name_last)\n",
    "alcon.evaluation( results )\n",
    "\n",
    "pr.disable()\n",
    "pstats.Stats(pr).sort_stats('tottime').print_stats(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         672751 function calls (654781 primitive calls) in 25.370 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 968 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      230   23.824    0.104   23.824    0.104 {imread}\n",
      "        3    0.123    0.041    0.123    0.041 {built-in method _pywrap_tensorflow_internal.TF_ExtendGraph}\n",
      "        5    0.112    0.022    0.112    0.022 {built-in method _pywrap_tensorflow_internal.TF_Run}\n",
      "     1390    0.107    0.000    0.120    0.000 /root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1202(<listcomp>)\n",
      "1105/1097    0.097    0.000    0.973    0.001 /root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:289(apply_op)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7ff081e35a20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テスト　imread\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "datasetdir = \"/share/alcon/dataset/\"\n",
    "annotation_name = \"test_5\"\n",
    "\"\"\"\n",
    "評価コードのメイン\n",
    ":param datasetdir データセットを格納したディレクトリへのパス\n",
    "\"\"\"\n",
    "\n",
    "# 初期化\n",
    "alcon = AlconUtils(datasetdir)\n",
    "myalgorithm = MyAlgorithm(datasetdir)\n",
    "\n",
    "# ターゲットの読み込み\n",
    "file_name_last = \"_lv1_\" + annotation_name + \".csv\"\n",
    "alcon.load_annotations_target(\"target\" + file_name_last)\n",
    "\n",
    "imgs = {}\n",
    "results = {}\n",
    "# １ターゲットずつ認識していくループ\n",
    "for bb_id, target in alcon.targets.items():\n",
    "    img_file_id, *bb = target\n",
    "    # ページ全体の画像\n",
    "    imgs[bb_id] = cv2.imread( os.path.join(datasetdir, \"images\", img_file_id+\".jpg\") )\n",
    "\n",
    "pr.disable()\n",
    "pstats.Stats(pr).sort_stats('tottime').print_stats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results: 111 / 230\n",
      "score: 48.260870\n",
      "         862912 function calls (844466 primitive calls) in 6.569 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 1002 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      230    3.153    0.014    3.153    0.014 {bilateralFilter}\n",
      "        4    1.107    0.277    1.107    0.277 {built-in method _pywrap_tensorflow_internal.TF_ExtendGraph}\n",
      "      235    0.766    0.003    0.766    0.003 {built-in method _pywrap_tensorflow_internal.TF_Run}\n",
      "1106/1098    0.098    0.000    0.874    0.001 /root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:289(apply_op)\n",
      "     1274    0.051    0.000    0.051    0.000 {built-in method _pywrap_tensorflow_internal.RunCppShapeInference}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7f619e4dbd68>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テストpredict\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "datasetdir = \"/share/alcon/dataset/\"\n",
    "\"\"\"\n",
    "評価コードのメイン\n",
    ":param datasetdir データセットを格納したディレクトリへのパス\n",
    "\"\"\"\n",
    "\n",
    "# 初期化\n",
    "alcon = AlconUtils(datasetdir)\n",
    "myalgorithm = MyAlgorithm(datasetdir)\n",
    "\n",
    "# ターゲットの読み込み\n",
    "alcon.load_annotations_target(\"target_lv1_test_5.csv\")\n",
    "\n",
    "for bb_id, target in alcon.targets.items():\n",
    "    img_file_id, *bb = target\n",
    "    # 認識処理（ここで各自のアルゴリズムを呼び出します）\n",
    "    # boundingbox id と紐付けて保存\n",
    "    results[bb_id] = myalgorithm.predict(imgs[bb_id], bb)\n",
    "\n",
    "\n",
    "# 評価\n",
    "alcon.load_annotations_ground(\"groundtruth\" + file_name_last)\n",
    "alcon.evaluation( results )\n",
    "\n",
    "pr.disable()\n",
    "pstats.Stats(pr).sort_stats('tottime').print_stats(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dataso', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c53f4e423b6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for bb_id, target in alcon.targets.items():\n",
    "    i += 1\n",
    "    if i > 350*4: break\n",
    "    img_file_id, *bb = target\n",
    "    if i % 50 == 1: print(i)\n",
    "    # ページ全体の画像\n",
    "    imgs[bb_id] = cv2.imread( os.path.join(datasetdir, \"images\", img_file_id+\".jpg\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = datagen.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9c7d6b741d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m    \u001b[0;31m#     continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mimg_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_filename_char\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbb_id\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mimg_filename\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyAlgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('full_dataset0.9', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fef6ff19c50>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCSNKPRFRNKIQl9EJI38f6DGMbZGeiPiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefb801b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
