{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.externals import joblib\n",
    "from user_function import MyAlgorithm\n",
    "from alcon_utils import AlconUtils\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow\n",
    "import cProfile, pstats\n",
    "\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "config = tensorflow.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.93\n",
    "keras.backend.tensorflow_backend.set_session(tensorflow.Session(config=config))\n",
    "\n",
    "# 初期化\n",
    "datasetdir = \"/share/alcon/dataset/\"\n",
    "alcon = AlconUtils(datasetdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         482548 function calls in 113.226 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 51 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    12998   89.744    0.007   89.744    0.007 {bilateralFilter}\n",
      "    12998   22.272    0.002   22.272    0.002 {imread}\n",
      "    12998    0.197    0.000    0.197    0.000 {cvtColor}\n",
      "    12998    0.196    0.000    0.196    0.000 {threshold}\n",
      "        1    0.171    0.171  113.007  113.007 <ipython-input-45-3403e1b54a1d>:9(<module>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7f5c1f7a9710>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "# アノテーションの読み込み\n",
    "alcon.load_annotations_target(\"target_lv1_iiyama_0.9.csv\")\n",
    "alcon.load_annotations_ground(\"groundtruth_lv1_iiyama_0.9.csv\")\n",
    "\n",
    "dataset = {}\n",
    "for bb_id, target in alcon.targets.items():\n",
    "    code = alcon.ground_truth[bb_id][0]\n",
    "    if code not in dataset:\n",
    "        dataset[code] = []\n",
    "    if len(dataset[code]) == 300:\n",
    "        continue\n",
    "    img_filename = alcon.get_filename_char( bb_id )\n",
    "    img = cv2.imread( img_filename )\n",
    "    feature = MyAlgorithm.feature_extraction(img)\n",
    "    dataset[code].append(feature)\n",
    "\n",
    "labels = []\n",
    "data = []\n",
    "classes = sorted(dataset.keys())\n",
    "\n",
    "for label, values in dataset.items():\n",
    "    labels += [classes.index(label)] * len(values)\n",
    "    data += values\n",
    "\n",
    "num_classes = 46\n",
    "input_shape = (32, 32, 1)# img_rows img_cols channel\n",
    "\n",
    "x_data = np.asarray(data).reshape(len(data), *input_shape)\n",
    "y_train = keras.utils.to_categorical( labels, num_classes )\n",
    "datasetdir = \"/share/alcon/dataset/\"\n",
    "annotation_name = \"test_5\"\n",
    "\n",
    "# 初期化\n",
    "alcon = AlconUtils(datasetdir)\n",
    "myalgorithm = MyAlgorithm(datasetdir)\n",
    "\n",
    "# ターゲットの読み込み\n",
    "file_name_last = \"_lv1_\" + annotation_name + \".csv\"\n",
    "alcon.load_annotations_target(\"target\" + file_name_last)\n",
    "\n",
    "pr.disable()\n",
    "pstats.Stats(pr).sort_stats('tottime').print_stats(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         863232 function calls (845262 primitive calls) in 2.916 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 1033 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.416    0.416    0.417    0.417 /root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/scipy/linalg/decomp_svd.py:16(svd)\n",
      "        3    0.210    0.070    0.210    0.070 {built-in method _pywrap_tensorflow_internal.TF_ExtendGraph}\n",
      "      581    0.178    0.000    0.197    0.000 {built-in method numpy.core.multiarray.array}\n",
      "        1    0.164    0.164    0.164    0.164 {built-in method _pickle.load}\n",
      "        3    0.156    0.052    0.156    0.052 {built-in method numpy.core.multiarray.dot}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7f61bdf48f98>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#　学習機に掛ける前の準備\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "config = tensorflow.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.93\n",
    "keras.backend.tensorflow_backend.set_session(tensorflow.Session(config=config))\n",
    "\n",
    "# 初期化\n",
    "datasetdir = \"/share/alcon/dataset/\"\n",
    "alcon = AlconUtils(datasetdir)\n",
    "\n",
    "# アノテーションの読み込み\n",
    "alcon.load_annotations_target(\"target_lv1_iiyama_0.9.csv\")\n",
    "alcon.load_annotations_ground(\"groundtruth_lv1_iiyama_0.9.csv\")\n",
    "\n",
    "dataset = {}\n",
    "with open('full_dataset0.9', mode='rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "labels = []\n",
    "data = []\n",
    "classes = sorted(dataset.keys())\n",
    "\n",
    "for label, values in dataset.items():\n",
    "    labels += [classes.index(label)] * len(values)\n",
    "    data += values\n",
    "\n",
    "num_classes = 46\n",
    "input_shape = (32, 32, 1)# img_rows img_cols channel\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    zca_whitening=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='constant',\n",
    "    cval=0.)\n",
    "x_train = np.asarray(data).reshape(len(data), *input_shape)\n",
    "y_train = keras.utils.to_categorical( labels, num_classes )\n",
    "datagen.fit(x_train)\n",
    "datasetdir = \"/share/alcon/dataset/\"\n",
    "annotation_name = \"test_5\"\n",
    "\n",
    "# 初期化\n",
    "alcon = AlconUtils(datasetdir)\n",
    "myalgorithm = MyAlgorithm(datasetdir)\n",
    "\n",
    "# ターゲットの読み込み\n",
    "file_name_last = \"_lv1_\" + annotation_name + \".csv\"\n",
    "alcon.load_annotations_target(\"target\" + file_name_last)\n",
    "\n",
    "pr.disable()\n",
    "pstats.Stats(pr).sort_stats('tottime').print_stats(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error when checking model input: data should be a Numpy array, or list/dict of Numpy arrays. Found: <keras.preprocessing.image.ImageDataGenerator object at 0x7f5c1f9dc160>...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-d9e1a3850596>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m classifier.fit(x_data, y_train, batch_size= 84, epochs=50,\n\u001b[1;32m     20\u001b[0m                       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                       validation_data=(x_data, y_train))\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1430\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1303\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1306\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1307\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     97\u001b[0m                             \u001b[0;34m': data should be a Numpy array, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                             \u001b[0;34m'or list/dict of Numpy arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                             'Found: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# Case: model expects multiple inputs but only received\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Error when checking model input: data should be a Numpy array, or list/dict of Numpy arrays. Found: <keras.preprocessing.image.ImageDataGenerator object at 0x7f5c1f9dc160>..."
     ]
    }
   ],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "classifier = keras.models.Sequential()\n",
    "classifier.add(keras.layers.normalization.BatchNormalization(input_shape = input_shape))\n",
    "classifier.add(Conv2D(32, kernel_size=(3,3), activation='relu')) # 30*30\n",
    "classifier.add(Conv2D(64,                 (3,3), activation='relu')) # 28*28 \n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(MaxPooling2D(pool_size=(4,4)))                      # 7*7\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(480, activation='relu'))\n",
    "classifier.add(Dropout(7/24))\n",
    "classifier.add(Dense(128, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "classifier.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Nadam(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "classifier.fit(x_data, y_train, batch_size= 84, epochs=50,\n",
    "                      callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)],\n",
    "                      validation_data=(x_data, y_train))\n",
    "classifier.fit_generator(datagen.flow(X_train, Y_train, batch_size=84),\n",
    "                        samples_per_epoch=len(X_train),\n",
    "                        nb_epoch=nb_epoch)\n",
    "\n",
    "joblib.dump(classes, \"./model.pkl\")\n",
    "classifier.save(\"./model2.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for bb_id, target in alcon.targets.items():\n",
    "    img_file_id, *bb = target\n",
    "    # 認識処理（ここで各自のアルゴリズムを呼び出します）\n",
    "    # boundingbox id と紐付けて保存\n",
    "    results[bb_id] = myalgorithm.predict(imgs[bb_id], bb)\n",
    "\n",
    "\n",
    "# 評価\n",
    "alcon.load_annotations_ground(\"groundtruth\" + file_name_last)\n",
    "alcon.evaluation( results )\n",
    "\n",
    "pr.disable()\n",
    "pstats.Stats(pr).sort_stats('tottime').print_stats(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         674686 function calls (656716 primitive calls) in 22.567 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 968 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      230   20.216    0.088   20.216    0.088 {imread}\n",
      "        3    0.872    0.291    0.872    0.291 {built-in method _pywrap_tensorflow_internal.TF_ExtendGraph}\n",
      "        5    0.266    0.053    0.266    0.053 {built-in method _pywrap_tensorflow_internal.TF_Run}\n",
      "1105/1097    0.099    0.000    0.861    0.001 /root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:289(apply_op)\n",
      "     1390    0.049    0.000    0.056    0.000 /root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:96(_extract_stack)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7f5d0c67ab70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テスト　imread\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "datasetdir = \"/share/alcon/dataset/\"\n",
    "annotation_name = \"test_5\"\n",
    "\"\"\"\n",
    "評価コードのメイン\n",
    ":param datasetdir データセットを格納したディレクトリへのパス\n",
    "\"\"\"\n",
    "\n",
    "# 初期化\n",
    "alcon = AlconUtils(datasetdir)\n",
    "myalgorithm = MyAlgorithm(datasetdir)\n",
    "\n",
    "# ターゲットの読み込み\n",
    "file_name_last = \"_lv1_\" + annotation_name + \".csv\"\n",
    "alcon.load_annotations_target(\"target\" + file_name_last)\n",
    "\n",
    "imgs = {}\n",
    "results = {}\n",
    "# １ターゲットずつ認識していくループ\n",
    "for bb_id, target in alcon.targets.items():\n",
    "    img_file_id, *bb = target\n",
    "    # ページ全体の画像\n",
    "    imgs[bb_id] = cv2.imread( os.path.join(datasetdir, \"images\", img_file_id+\".jpg\") )\n",
    "\n",
    "pr.disable()\n",
    "pstats.Stats(pr).sort_stats('tottime').print_stats(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results: 195 / 230\n",
      "score: 84.782609\n",
      "         898856 function calls (880410 primitive calls) in 7.065 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 1002 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      230    3.099    0.013    3.099    0.013 {bilateralFilter}\n",
      "        4    1.520    0.380    1.520    0.380 {built-in method _pywrap_tensorflow_internal.TF_ExtendGraph}\n",
      "      235    0.841    0.004    0.841    0.004 {built-in method _pywrap_tensorflow_internal.TF_Run}\n",
      "1106/1098    0.107    0.000    0.930    0.001 /root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:289(apply_op)\n",
      "     1391    0.051    0.000    0.058    0.000 /root/.pyenv/versions/3.6.1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:96(_extract_stack)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7f5d0d3729e8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テストpredict\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "datasetdir = \"/share/alcon/dataset/\"\n",
    "\"\"\"\n",
    "評価コードのメイン\n",
    ":param datasetdir データセットを格納したディレクトリへのパス\n",
    "\"\"\"\n",
    "\n",
    "# 初期化\n",
    "alcon = AlconUtils(datasetdir)\n",
    "myalgorithm = MyAlgorithm(datasetdir)\n",
    "\n",
    "# ターゲットの読み込み\n",
    "alcon.load_annotations_target(\"target_lv1_test_5.csv\")\n",
    "\n",
    "for bb_id, target in alcon.targets.items():\n",
    "    img_file_id, *bb = target\n",
    "    # 認識処理（ここで各自のアルゴリズムを呼び出します）\n",
    "    # boundingbox id と紐付けて保存\n",
    "    results[bb_id] = myalgorithm.predict(imgs[bb_id], bb)\n",
    "\n",
    "\n",
    "# 評価\n",
    "alcon.load_annotations_ground(\"groundtruth\" + file_name_last)\n",
    "alcon.evaluation( results )\n",
    "\n",
    "pr.disable()\n",
    "pstats.Stats(pr).sort_stats('tottime').print_stats(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dataso', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c53f4e423b6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for bb_id, target in alcon.targets.items():\n",
    "    i += 1\n",
    "    if i > 350*4: break\n",
    "    img_file_id, *bb = target\n",
    "    if i % 50 == 1: print(i)\n",
    "    # ページ全体の画像\n",
    "    imgs[bb_id] = cv2.imread( os.path.join(datasetdir, \"images\", img_file_id+\".jpg\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = datagen.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9c7d6b741d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m    \u001b[0;31m#     continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mimg_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_filename_char\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbb_id\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mimg_filename\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyAlgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('full_dataset0.9', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4042",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-961f0b31a7ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4042\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 4042"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
